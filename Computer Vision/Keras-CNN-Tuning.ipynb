{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Keras\n",
    "\n",
    "Let's use Keras on the MNIST handwriting data set, using a Convolutional Neural Network that's suited for image processing. CNN's are less sensitive to where in the image the pattern is that we're looking for.\n",
    "\n",
    "We'll start by importing the stuff we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up our raw data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're treating the data as 2D images of 28x28 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale. If we were dealing with color images, it would be 3 instead of 1 since we'd have red, green, and blue color channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our train and test labels to be categorical in one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check let's print out one of the training images with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the meat of the problem. Setting up a convolutional neural network involves more layers.\n",
    "\n",
    "We'll start with a 2D convolution of the image - it's set up to take 32 windows, or \"filters\", of each image, each filter being 3x3 in size.\n",
    "\n",
    "We then run a second convolution on top of that with 64 3x3 windows - this topology is just what comes recommended within Keras's own examples. Again you want to re-use previous research whenever possible while tuning CNN's, as it is hard to do.\n",
    "\n",
    "Next we apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
    "\n",
    "Next we flatten the 2D layer we have at this stage into a 1D layer. So at this point we can just pretend we have a traditional multi-layer perceptron...\n",
    "\n",
    "... and feed that into a hidden, flat layer of 128 units.\n",
    "\n",
    "\n",
    "And finally, we feed that into our final 10 units where softmax is applied to choose our category of 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 17:56:47.137592: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the model description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 12, 12, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing multiple categorization, so categorical_crossentropy is still the right loss function to use. We'll use the Adam optimizer, although the example provided with Keras uses RMSProp. You might want to try both if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train our model... to make things go a little faster, we'll use batches of 32.\n",
    "\n",
    "## Warning\n",
    "\n",
    "This will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 54s - loss: 0.1144 - accuracy: 0.9646 - val_loss: 0.0465 - val_accuracy: 0.9834 - 54s/epoch - 29ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 56s - loss: 0.0386 - accuracy: 0.9880 - val_loss: 0.0350 - val_accuracy: 0.9888 - 56s/epoch - 30ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 72s - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.0393 - val_accuracy: 0.9884 - 72s/epoch - 39ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 57s - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0348 - val_accuracy: 0.9906 - 57s/epoch - 30ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 54s - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0471 - val_accuracy: 0.9873 - 54s/epoch - 29ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 53s - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0459 - val_accuracy: 0.9885 - 53s/epoch - 28ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 58s - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0390 - val_accuracy: 0.9909 - 58s/epoch - 31ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 63s - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0532 - val_accuracy: 0.9877 - 63s/epoch - 33ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 58s - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9901 - 58s/epoch - 31ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 56s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0550 - val_accuracy: 0.9878 - 56s/epoch - 30ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can see that we started overfitting pretty early on, as our accuracy on the test set started exceeding our accuracy on the validation set. Our validation accuracy maxed out at around 99.0% after just a couple of epochs, while our accuracy on the test set kept climbing.\n",
    "\n",
    "To prevent overfitting, we need to perform some sort of regularization. Dropout layers are one such technique in deep learning; they work by \"dropping out\" neurons on each pass to force learning to spread itself out across the network as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    # 64 3x3 kernels\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    # Reduce by taking the max of each 2x2 block\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Dropout to avoid overfitting\n",
    "    model.add(Dropout(0.25))\n",
    "    # Flatten the results to one dimension for passing into our final layer\n",
    "    model.add(Flatten())\n",
    "    # A hidden layer to learn with\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Another dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    # Final categorization from 0-9 with softmax\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = MakeModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it again with those two dropout layers added in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 67s - loss: 0.1786 - accuracy: 0.9455 - val_loss: 0.0419 - val_accuracy: 0.9873 - 67s/epoch - 36ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 74s - loss: 0.0777 - accuracy: 0.9767 - val_loss: 0.0378 - val_accuracy: 0.9874 - 74s/epoch - 39ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 74s - loss: 0.0592 - accuracy: 0.9810 - val_loss: 0.0319 - val_accuracy: 0.9900 - 74s/epoch - 40ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 72s - loss: 0.0506 - accuracy: 0.9847 - val_loss: 0.0324 - val_accuracy: 0.9906 - 72s/epoch - 39ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 71s - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.0328 - val_accuracy: 0.9906 - 71s/epoch - 38ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 73s - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.0277 - val_accuracy: 0.9910 - 73s/epoch - 39ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 70s - loss: 0.0334 - accuracy: 0.9892 - val_loss: 0.0306 - val_accuracy: 0.9922 - 70s/epoch - 37ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 74s - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.0285 - val_accuracy: 0.9912 - 74s/epoch - 40ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 72s - loss: 0.0256 - accuracy: 0.9919 - val_loss: 0.0329 - val_accuracy: 0.9907 - 72s/epoch - 39ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 80s - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.0276 - val_accuracy: 0.9928 - 80s/epoch - 43ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better; our train and test accuracy ended up about the same, at 99.2%. There may still be a tiny bit of overfitting going on, but it's a lot better.\n",
    "\n",
    "Let's also explore the effect the batch size has; as an experiment, let's increase it up to 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 - 36s - loss: 0.5993 - accuracy: 0.8224 - val_loss: 0.1498 - val_accuracy: 0.9560 - 36s/epoch - 606ms/step\n",
      "Epoch 2/10\n",
      "60/60 - 35s - loss: 0.1635 - accuracy: 0.9524 - val_loss: 0.0700 - val_accuracy: 0.9777 - 35s/epoch - 588ms/step\n",
      "Epoch 3/10\n",
      "60/60 - 41s - loss: 0.1026 - accuracy: 0.9700 - val_loss: 0.0492 - val_accuracy: 0.9844 - 41s/epoch - 682ms/step\n",
      "Epoch 4/10\n",
      "60/60 - 42s - loss: 0.0767 - accuracy: 0.9777 - val_loss: 0.0386 - val_accuracy: 0.9866 - 42s/epoch - 698ms/step\n",
      "Epoch 5/10\n",
      "60/60 - 45s - loss: 0.0643 - accuracy: 0.9807 - val_loss: 0.0357 - val_accuracy: 0.9878 - 45s/epoch - 744ms/step\n",
      "Epoch 6/10\n",
      "60/60 - 39s - loss: 0.0555 - accuracy: 0.9822 - val_loss: 0.0334 - val_accuracy: 0.9883 - 39s/epoch - 646ms/step\n",
      "Epoch 7/10\n",
      "60/60 - 37s - loss: 0.0505 - accuracy: 0.9847 - val_loss: 0.0300 - val_accuracy: 0.9903 - 37s/epoch - 625ms/step\n",
      "Epoch 8/10\n",
      "60/60 - 38s - loss: 0.0439 - accuracy: 0.9866 - val_loss: 0.0287 - val_accuracy: 0.9906 - 38s/epoch - 627ms/step\n",
      "Epoch 9/10\n",
      "60/60 - 37s - loss: 0.0410 - accuracy: 0.9872 - val_loss: 0.0290 - val_accuracy: 0.9905 - 37s/epoch - 619ms/step\n",
      "Epoch 10/10\n",
      "60/60 - 37s - loss: 0.0383 - accuracy: 0.9887 - val_loss: 0.0295 - val_accuracy: 0.9901 - 37s/epoch - 624ms/step\n"
     ]
    }
   ],
   "source": [
    "model = MakeModel()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=1000,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this block a few times, you'll probably get very different results. Large batch sizes tend to get stuck in \"local minima\", and converge on the wrong solution at random. Smaller batch sizes also have a regularization effect. Sometimes you'll get lucky and the large batch will converge on a good solution; other times, not so much.\n",
    "\n",
    "Let's explore the effect of the learning rate. The default learning rate for Adam is 0.001; let's see what happens if we increase it by an order of magnitude to 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MakeModel()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "adam = tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 83s - loss: 0.2617 - accuracy: 0.9238 - val_loss: 0.0894 - val_accuracy: 0.9727 - 83s/epoch - 44ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 87s - loss: 0.1867 - accuracy: 0.9477 - val_loss: 0.0651 - val_accuracy: 0.9807 - 87s/epoch - 46ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 75s - loss: 0.1819 - accuracy: 0.9498 - val_loss: 0.0668 - val_accuracy: 0.9809 - 75s/epoch - 40ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 75s - loss: 0.1774 - accuracy: 0.9520 - val_loss: 0.0720 - val_accuracy: 0.9787 - 75s/epoch - 40ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 86s - loss: 0.1766 - accuracy: 0.9528 - val_loss: 0.0665 - val_accuracy: 0.9800 - 86s/epoch - 46ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 74s - loss: 0.1819 - accuracy: 0.9510 - val_loss: 0.0785 - val_accuracy: 0.9797 - 74s/epoch - 40ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 86s - loss: 0.1709 - accuracy: 0.9552 - val_loss: 0.0704 - val_accuracy: 0.9792 - 86s/epoch - 46ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 74s - loss: 0.1632 - accuracy: 0.9570 - val_loss: 0.0841 - val_accuracy: 0.9800 - 74s/epoch - 40ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 77s - loss: 0.1695 - accuracy: 0.9555 - val_loss: 0.0808 - val_accuracy: 0.9799 - 77s/epoch - 41ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 76s - loss: 0.1729 - accuracy: 0.9550 - val_loss: 0.1173 - val_accuracy: 0.9722 - 76s/epoch - 41ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That had a huge, and terrible, effect on the results. Small batch sizes are best paired with low learning rates, and large learning rates have a tendency to overshoot the correct solution entirely - which is probably what happened here. The learning rate is an example of a hyperparameter that you might want to tune by just trying different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
